# CX Configuration
# See https://cxlinux-ai.com/docs/configuration for details

# LLM Provider Settings
llm:
  # Default provider: ollama (local), anthropic, openai
  default_provider: ollama

  # Ollama settings (local LLM)
  ollama:
    host: http://localhost:11434
    model: llama3.2:3b
    timeout: 120

  # API providers (requires API keys in environment or ~/.cx/env)
  # anthropic:
  #   model: claude-3-sonnet-20240229
  # openai:
  #   model: gpt-4-turbo-preview

# Security Settings
security:
  # Enable sandbox for package operations
  sandbox: true
  sandbox_provider: firejail

  # Require confirmation for system changes
  confirm_changes: true

  # Maximum packages per operation
  max_packages_per_operation: 20

# Logging
logging:
  level: INFO
  file: /var/log/cx/cx.log
  max_size_mb: 50
  backup_count: 5

# Cache Settings
cache:
  enabled: true
  directory: /var/cache/cx
  ttl_hours: 24

# Hardware Detection
hardware:
  auto_detect: true
  gpu_support: true
